!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Kalman Filter-Inspired Visual-Inertial Odometry with Motion Priors</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <!--<h3 style="font-size: 1.5rem; font-weight: bold; color: black; margin-top: 20px;">Methods</h3>-->

  <!-- KaTeX CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <!-- KaTeX auto-render extension CSS (optional) -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.css">
  
  <!-- KaTeX JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <!-- KaTeX auto-render extension JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body, {delimiters: [{left: '$', right: '$', display: false}, {left: '$$', right: '$$', display: true}]});"></script>

  
</head>

<!-- Paper authors -->
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><br>Kalman Filter-Inspired Visual-Inertial Odometry with Motion Priors </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/minje-KIM" target="_blank">Minje Kim</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://github.com/Kang-ChangWoo" target="_blank">Changwoo Kang</a><sup>1*</sup>,</span>
                  <span class="author-block">
                    <a href="http://rml.unist.ac.kr/" target="_blank">Jeong hwan Jeon</a><sup>1</sup>,</span>
                          <span class="author-block">
                            <a href="https://unist.info/" target="_blank">Kyungdon Joo</a><sup>1✝</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>UNIST<br>Under Review</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>✝</sup>Corresponding Author</small></span>
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
              <br> 
              <p style="text-align: justify;">
              Visual-inertial odometry (VIO) is a critical component in robotics applications, enabling accurate motion estimation. 
              However, recent VIO methods rely on short time interval sensor readings without contextual understanding, limiting their ability to capture proper motion dynamics. 
              Inspired by the estimation process of the Kalman filter, we propose a novel Kalman filter-inspired VIO framework consisting of multiple transition models to capture actual motion dynamics. 
                Specifically, we present a movement classifier that categorizes the movement of the agent from IMU readings for adaptively fusing multiple motion priors. 
                We decouple the given IMU readings into angular velocity and acceleration to input them into encoders tailored for rotational and translational characteristics by eliminating unnecessary interference. 
                Furthermore, we introduce a regularization loss that induces the proposed learnable transition model to align with the properties of the Kalman system equations. 
                Experimental results on the KITTI and nuScenes datasets show that our method significantly improves rotation estimation accuracy and maintains competitive translation estimation performance.
             </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<div class="row mt-5" style="text-align: center;">
  <div class="container is-max-desktop">
    <div class="has-text-centered">
      <div class="col">
        <div class="wider-element">
          <h4 class="title is-4"> Schematic illustration of motion-adaptive transition model (teaser) </h4> 
          <br>   
          <img src="./static/images/fig1.png" width="500px">
          <br>
          <p style="text-align: justify;">
            The classical transition model infers a single pose uncertainty along the trajectory (dotted ellipsoids). 
            In contrast, our model predicts multiple pose uncertainties (each colored ellipsoid) based on motion priors. 
            Individual pose uncertainties are fused into a single, combined uncertainty based on movement probabilities (e.g., left turn, straight movement, right turn).
            </p>
          <br><br><br>
          </div>
        </div>
      </div>
    </div>
  </div>
  <hr>

  
  <br> 
  <!--<br><hr>-->
  <div class="row mt-5">
    <div class="container is-max-desktop">
    <div class="has-text-centered">
    <div class="col">
      <h2 class="title is-3">Methods</h2>
      <br> 
      <h4 class="title is-4">  Illustration Kalman filter-inspired architecture </h4> 
      <p style="text-align: justify;">
      </p>
      <img src="./static/images/fig2.png" width="900px" >
      <br>
      <p style="text-align: justify;">
      The observation model (top box) processes images $\{{\mathbf{I}_{t-1}, \mathbf{I}_{t}}\}$ and IMU data $\mathbf{M}_{t-1\rightarrow t}$ to estimate the observation state $\mathbf{z}_t$, and the transition model (bottom box) incorporates both angular velocity $\omega_{t-1 \rightarrow t}$ and acceleration $a_{t-1 \rightarrow t}$ to predict the predicted state $\hat{\mathbf{x}}_t^{\prime}$. 
        The Kalman update step (right box) fuses each state using a Kalman gain $\mathbf{K}_t$, resulting in the updated state $\hat{\mathbf{x}}_t$.
      </p>
      <br><br><br><br>
      <h4 class="title is-4">  Architecture of transition model</h4>
      <img src="./static/images/fig3.png" width="600px">
      <br>
      <p style="text-align: justify;">
        Our transition model separately processes angular velocity $\omega$ and acceleration $a$ through each LSTM and incorporates the previous state information into the MLP. 
        The fused vectors are then split and passed through separate MLPs to predict the rotation $\phi$, translation $\mathbf{v}$, and the associated uncertainty matrices $\mathbf{A}$ and $\mathbf{Q}$.    
      </p>
      
      <br><br><br>

      <hr>
      <br>
      <h2 class="title is-2"> Experimental Results</h2> 
      <p class="text-justify">
        <br>
         <h4 class="title is-4"> Qualitative comparisons on the KITTI dataset</h4>
        <img src="./static/images/qual1.png" width="700px">
        <br>
        <p class="text-center">
          We present the trajectories projected onto the XZ plane, representing the poses of agent on the ground at each time step (top). 
          We also illustrate the vertical displacement over time (bottom). 
          Our proposed method (red line) exhibits a tendency to stay close to the ground truth (black dotted line).
        </p>
      
      <br><br><br><br>
        <h4 class="title is-4"> Results of the movement classifier on the KITTI dataset</h4> 
        <img src="./static/images/qual2.png" class="responsive-img" width="700px">
        <br>
        <p class="text-center">
          We color the highest logit of the classifier at each pose: red for left turns, green for straight movement, and blue for right turns. Trajectories are projected onto the XZ plane. We conduct a study on the KITTI dataset.
        </p>
        <br><br><br><br>
      
        <!-- <h5>Implicit Decoder</h5>  -->
        <h4 class="title is-4"> Qualitative comparison on the nuScenes dataset</h4>  
        <img src="./static/images/qual3.png" class="responsive-img" width="700px">
        <br>
        <p class="text-center">
          We present the trajectories projected onto the XY plane, representing the poses of agent on the ground at each time step. Our proposed method (red line) exhibits a tendency to stay close to the ground truth (black dotted line).
        </p>
        <br><br><br><br>

      
        <!-- <h5>Implicit Decoder</h5> -->
        <h4 class="title is-4"> Quantitative comparison on the KITTI dataset</h4>
        <img src="./static/images/quan1.png" class="responsive-img" width="850px">
        <br><br><br><br>
      
        <!-- <h5>Implicit Decoder</h5> -->
        <h4 class="title is-4"> Quantitative comparison on the nuScenes dataset</h4>
        <img src="./static/images/quan2.png" class="responsive-img" width="850px">
        <br><br><br><br>
       </div>
     </div>
    </div>
  </div>

<hr>
<!-- Teaser video-->
<section class="section hero is-light">
  <div class="container is-max-desktop" style="text-align: center;">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Supplementary video</h2>
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="static/videos/supple_video.mp4"type="video/mp4">
          </video>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->
  
  
<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
