<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Beyond the Highlights: Video Retrieval with Salient and Surrounding Contexts (WACV 2026)">
  <meta name="keywords" content="Video Retrieval, SS Datasets, Multimodal, WACV 2026, Scene Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Beyond the Highlights: Video Retrieval with Salient and Surrounding Contexts</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <style>
    body { font-family: 'Noto Sans', sans-serif; }
    .title.is-1 { font-family: 'Google Sans', sans-serif; font-weight: 700; }
    .section-title { font-family: 'Google Sans', sans-serif; font-weight: 600; font-size: 2rem; margin-bottom: 1.5rem; text-align: left; border-bottom: 2px solid #dbdbdb; padding-bottom: 5px;}
    .publication-venue { font-size: 1.2rem; font-weight: bold; color: #006400; margin-top: 10px; }
    
    /* Highlight Colors */
    .highlight-text { color: #d63031; font-weight: bold; } /* Red for Salient */
    .surround-text { color: #0984e3; font-weight: bold; } /* Blue for Surround */
    
    /* Clean Image Style (No Shadows, No Borders) */
    .img-fluid { 
      max-width: 100%; 
      height: auto; 
      display: block; 
      margin: 0 auto;
    }
    
    /* Caption Style - 양쪽 정렬(Justify) 적용 */
    .caption { 
      font-size: 1.2rem; 
      color: #4a4a4a; 
      text-align: justify; /* Center에서 Justify로 변경 */
      margin-top: 15px; 
      margin-bottom: 30px;
      font-weight: 500;
    }

    /* 본문 양쪽 정렬 적용 */
    .content p { 
        font-size: 1.1rem; 
        line-height: 1.6; 
        text-align: justify; /* 양쪽 정렬 추가 */
    }
    .box-gray { background-color: #f9f9f9; border-radius: 0px; padding: 30px; margin-bottom: 30px; }
    .bibtex-section { background-color: #f5f5f5; padding: 20px; font-family: Courier New; font-size: 0.95rem; line-height: 1.5; overflow-x: auto; border-radius: 5px;}
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Beyond the Highlights: Video Retrieval with <br> Salient and Surrounding Contexts</h1>
          <div class="publication-venue">WACV 2026</div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="devappendcbangj.github.io">Jaehun Bang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="mailto:ybmoon@postech.ac.kr">Ye-Bin Moon</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="mailto:taehyun.oh@kaist.ac.kr">Tae-Hyun Oh</a><sup>3*</sup>,</span>
            <span class="author-block">
              <a href="mailto:kyungdon@unist.ac.kr">Kyungdon Joo</a><sup>1*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UNIST,</span>
            <span class="author-block"><sup>2</sup>POSTECH,</span>
            <span class="author-block"><sup>3</sup>KAIST</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="far fa-images"></i></span>
                  <span>SS Datasets</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="has-text-centered">
        <img src="./images/f1_teaser.png" alt="Teaser Comparison" class="img-fluid" style="width: 75%;">
        <h2 class="subtitle has-text-centered" style="margin-top: 20px;">
          Existing datasets focus mainly on <b>Salient</b> events.<br> 
          Our <b>SS Datasets</b> capture fine-grained <span class="highlight-text"><b>Salient</b></span> and <span class="surround-text"><b>Surrounding</b></span> contexts<br>over semantically meaningful temporal segments.
        </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="section-title">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            When searching for videos, users often rely on <b>surrounding context</b> such as background elements or <b>temporal details</b> beyond salient content. However, existing video models struggle with fine-grained spatio-temporal understanding, particularly surrounding contexts, and there are no datasets that effectively evaluate their performance.
          </p>
          <p>
            We introduce <strong>SS Datasets</strong>, three video retrieval datasets with detailed salient and surrounding captions. To capture rich, temporally localized contexts aligned with <b>meaningful scene changes</b>, we segment videos by scene transitions and generate captions with a vision-language model. Analyzing current models reveals difficulties in handling surrounding queries and temporally complex videos. To address this, we propose simple yet effective baselines that improve retrieval across diverse query types, enabling more <b>robust generalization to real-world scenarios</b>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="section-title">The Challenge: Overcoming Salient Bias</h2>
    
    <div class="content has-text-justified mb-5">
      <p>
        Current video retrieval models exhibit a strong bias towards prominent foreground objects. 
        While they excel at identifying main actions, they often overlook <b>surrounding contexts</b> such as background details, weather, or subtle temporal cues.
      </p>
    </div>

    <div class="columns is-centered">
      <div class="column is-10 has-text-centered">
        <p class="caption">
          For instance, when a user queries for a background detail like a cushion, 
          standard zero-shot models focus entirely on the main subject, such as the dog, and fail to retrieve the correct clip.
        </p>
        <img src="./images/f2_motivation.png" alt="Motivation Failure Case" class="img-fluid" style="width: 80%;">
      </div>
    </div>

    <div class="columns is-centered mt-5">
      <div class="column is-10 has-text-centered">
        <p class="caption">
          This observation is supported by quantitative evidence. In zero-shot settings, performance drops drastically on surrounding queries compared to salient or original queries.
        </p>
        <img src="./images/t5_zero_shot_quantitative.png" alt="Zero-shot Performance Gap" class="img-fluid" style="width: 100%;">
      </div>
    </div>

  </div>
</section>

<section class="section" style="background-color: #fcfcfc;">
  <div class="container is-max-desktop">
    <h2 class="section-title">SS Datasets: Scale & Diversity</h2>
    
    <div class="content has-text-centered mb-5">
      <p>
        To address this limitation, we <b>extend MSRVTT, LSMDC, and DiDeMo</b> with dense, fine-grained captions that explicitly distinguish salient and surrounding contexts.
      </p>
    </div>

    <div class="columns is-centered">
        <div class="column is-12 has-text-centered">
            <img src="./images/t1_data_stat.png" alt="Dataset Statistics" class="img-fluid" style="width: 98%;">
            <p class="caption">
              Compared with original benchmarks, the SS Datasets include substantially more captions aligned with semantically meaningful temporal segments, enabling <b>richer contextual coverage</b>.
            </p>
        </div>
    </div>

    <br>

    <div class="columns is-centered">
      <div class="column is-10 has-text-centered">
        <img src="./images/t3_data_diversity.png" alt="Caption Diversity" class="img-fluid" style="width: 100%;">
        <p class="caption">
          Our captions exhibit <b>higher variance and distance</b> from the mean compared to original captions, indicating <b>richer linguistic diversity</b>.
        </p>
      </div>
    </div>

    <div class="columns is-centered mt-4">
      <div class="column is-12 has-text-centered">
        <img src="./images/ts2_data_diversity.png" alt="Semantic Distance" class="img-fluid" style="width: 98%;">
        <p class="caption">
          <b>Semantic analysis</b> shows that surrounding captions are clearly distinct from salient ones, indicating that our dataset <b>captures complementary information</b> instead of redundant descriptions.
        </p>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="section-title">In-depth Analysis</h2>
    
    <div class="content has-text-justified mb-5">
      <p style="font-size: 1.2rem;">
        Beyond standard retrieval evaluation, we analyze <b>how video properties</b>, such as temporal complexity, <b>affect model performance</b>.
      </p>
    </div>

    <div class="columns is-centered">
      <div class="column is-8 has-text-centered">
         <img src="./images/f3_property_analysis.png" alt="Performance Analysis" class="img-fluid" style="width: 90%;">
         <p class="caption">
           We found that <strong>more clips</strong> (higher temporal complexity) generally lead to <b>lower performance</b>, while <strong>longer clip duration</strong> helps models understand the context <b>better</b>.
         </p>
      </div>
    </div>

    <div class="columns is-centered mt-5">
      <div class="column is-14 has-text-centered">
        <img src="./images/fs2_correlation_matrix.png" alt="Correlation Matrix" class="img-fluid" style="width: 100%;">
        <p class="caption">
          A detailed <b>correlation matrix</b> further reveals the relationship between <b>video properties</b> (like clip count and duration) and <b>retrieval recall</b>.
        </p>
      </div>
    </div>

  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="section-title">Qualitative Results</h2>
    
    <div class="content has-text-justified mb-5">
      <p class="is-size-5" style="text-align: center;">
        Our simple baseline captures information that is often <b>underrepresented in existing approaches</b>.
      </p>
    </div>

    <div class="columns is-centered">
      <div class="column is-8 has-text-centered">
        <img src="./images/f4_qualitative.png" alt="Qualitative Results" class="img-fluid" style="width: 100%;">
        <p class="caption">
          While the zero-shot model does not capture the <b>specific background detail</b> of a white object with red, the baseline retrieves the correct clip containing the cushion.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="section-title">BibTeX</h2>
    <pre class="bibtex-section"><code>@inproceedings{bang2026beyond,
  title={Beyond the Highlights: Video Retrieval with Salient and Surrounding Contexts},
  author={Bang, Jaehun and Moon, Ye-Bin and Oh, Tae-Hyun and Joo, Kyungdon},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year={2026}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            Template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>