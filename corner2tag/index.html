<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>HUSH</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <!--<h3 style="font-size: 1.5rem; font-weight: bold; color: black; margin-top: 20px;">Methods</h3>-->

  <!-- KaTeX CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <!-- KaTeX auto-render extension CSS (optional) -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.css">
  
  <!-- KaTeX JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <!-- KaTeX auto-render extension JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body, {delimiters: [{left: '$', right: '$', display: false}, {left: '$$', right: '$$', display: true}]});"></script>

  
</head>

<!-- Paper authors -->
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><br>From Corners to Fiducial Tags: Revisiting Checkerboard Calibration for Event Cameras </h1>
            <div class="is-size-5 publication-authors">
              
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://taehun-ryu.github.io/" target="_blank">Taehun Ryu</a><sup>1</sup>,</span>&nbsp;&nbsp;&nbsp;
                <span class="author-block">
                  <a href="https://github.com/Kang-ChangWoo" target="_blank">Changwoo Kang</a><sup>1</sup>,</span>&nbsp;&nbsp;&nbsp;
                  <span class="author-block">
                            <a href="https://unist.info/" target="_blank">Kyungdon Joo</a><sup>1‚úù</sup>
                  </span>&nbsp;
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>3D Vision & Robotics Lab, UNIST</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <span class="eql-cntrb"><small><br> <sup>‚úù</sup>Corresponding Author</small></span>
                  </div>

              <!-- Conference -->
              <img src="./static/imgs/checkerboard.png" style="height: 1.1em; width: auto; vertical-aligh: middle; color: #3b7ccb;">
              <strong style="color: #3b7ccb;">CVPR 2026</strong><br>
            
                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- Paper PDF link -->
                      <!-- TODO: Update the link -->
                      <span class="link-block">
                        <!-- <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_HUSH_Holistic_Panoramic_3D_Scene_Understanding_using_Spherical_Harmonics_CVPR_2025_paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark"> -->
                        <a target="_blank" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- TODO: Update the link -->
                    <span class="link-block">
                      <!-- <a href="https://openaccess.thecvf.com/content/CVPR2025/supplemental/Lee_HUSH_Holistic_Panoramic_CVPR_2025_supplemental.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark"> -->
                      <a target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/taehun-ryu/corner2tag" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- TODO: Update the link -->
                <span class="link-block">
                  <!-- <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark"> -->
                  <a target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
              <br>
              <p style="text-align: justify;">
              <strong>TL;DR:</strong> We first present a novel calibration framework for event cameras that directly detects checkerboard corners and extends detection to fiducial markers like AprilTags!
              <br><br>
              üåü <strong>Key insight</strong>: At a checkerboard corner, event rate (ER) theoretically becomes zero, providing a precise and physically grounded anchor for calibration in event-based vision!
              </p>
              <br>
              <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
              <source src="./static/imgs/HUSH_motivation.mp4"type="video/mp4">
              </video> -->
              <br>
              <br>
              <h2 class="title is-3">Abstract</h2>
              <hr>
              <p style="text-align: justify;">
              The conventional checkerboard-based calibration for standard cameras faces fundamental limitations when applied to bio-inspired event cameras.
              Specifically, this stems from two challenges: (i) Events are triggered asynchronously at different timestamps along motion trajectories.
              If we accumulate them directly on the image plane, it causes temporal misalignment and produces blurred edges. Directly accumulating them on the image plane causes temporal misalignment and produces blurred edges.
              (ii) Checkerboard corners on event cameras show near-zero event occurrence at the corner itself.
              This hinders reliable corner localization and makes calibration difficult. To address these issues, we present a novel calibration framework that directly detects checkerboard corners from a raw event stream.
              We first mathematically analyze the absence of events at corner points. Based on this fact, we then leverage edge-driven event cues to initialize corner positions.
              Using the near-zero event occurrence at checkerboard corners, we gradually refine the estimated corner toward low event-density regions, achieving sub-pixel accuracy.
              Furthermore, we extend the corner detection to fiducial markers such as AprilTags, resulting in reliable detection even under partial visibility or occlusion.
              Evaluations on self-collected and public data demonstrate reliable checkerboard corner detection and stable camera calibration.
             </p>

             <br> 
             <!--<br><hr>-->
             <div class="row mt-5">
             <div class="container is-max-desktop">
             <div class="has-text-centered">
             <!-- <div class="col">
                <h2 class="title is-3">Methods</h2>
                <hr>
                <img src="./static/imgs/Fig_framework.jpg" width="1000px" >
                <p style="text-align: justify;">
                  HUSH first extracts multi-scale image features $f_i$ and scene-wise SH bases via feature extractor and SH coefficient network.
                  These image features and the scene-wise SH bases are then fed into the <strong>SH-based hierarchical attention module</strong> and the <strong>SH basis index module</strong> to estimate comprehensive scene feature $f_S$ and task-relevant features ($f_D$, $f_N$, $f_L$).
                  Finally, various scene-understanding tasks are performed as these features pass through task-specific heads.
                </p>
                <br><br><br> -->

             <br> 
             <!--<br><hr>-->
             <!-- <div class="row mt-5">
             <div class="container is-max-desktop">
             <div class="has-text-centered">
             <div class="col">
                <h2 class="title is-3">Task-relevant SH bases</h2>
                <hr>
                <br> 
                <p style="text-align: justify;">
                  To validate the effectiveness of using SH basis functions as queries, we compare the results from conventional learnable (LR) queries and our SH queries.
                  We visualize the frequently referred queries both on 2D and 3D domains w.r.t target task (depth/normal estimation).
                  As we can see below, using the SH basis function as a query can keep better geometric consistency of the scene than the LR query.
                </p>
                <br>
                <h5 class="title is-4"> Visualization on 2D </h5>
                <img src="./static/imgs/Fig_SHQ.jpg" width="900px" >
                <br><br><br>
                <h5 class="title is-4"> Visualization on 3D </h5>
                <img src="./static/imgs/Fig_SHQ_3D.jpg" width="900px" >
                <br><br><br> -->

            <br> 
            <!--<br><hr>-->
            <!-- <div class="row mt-5">
            <div class="container is-max-desktop">
            <div class="has-text-centered">
            <div class="col">
                <h2 class="title is-3">Results</h2>
                <hr>
                <br>
                <h5 class="title is-4"> Depth estimation </h5>
                <img src="./static/imgs/Fig_depth_comparison.jpg" width="900px" >
                <br><br><br>
                <h5 class="title is-4"> Surface normal estimation </h5>
                <img src="./static/imgs/Fig_normal_comparison.jpg" width="600px" >
                <br><br><br>
                <h5 class="title is-4"> Layout estimation </h5>
                <img src="./static/imgs/Fig_layout.jpg" width="900px" >
                <br><br><br>
                <h5 class="title is-4"> Comparison on 3D </h5>
                <img src="./static/imgs/Fig_3D.jpg" width="900px" >
                <br><br><br> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- TODO: update month and pages -->
  <hr style="margin-top: 0px">
  <pre style="background-color: #e9eeef; padding: 1.5em 1.5em; border-radius: 20px">
    <code>
      @InProceedings{Ryu_corner2tag,
        author    = {Taehun Ryu and Changwoo Kang and Kyungdon Joo},
        title     = {From Corners to Fiducial Tags: Revisiting Checkerboard Calibration for Event Cameras},
        booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
        year      = {2026},
       }
    </code>
  </pre>
  <hr>

  </body>
  </html>
